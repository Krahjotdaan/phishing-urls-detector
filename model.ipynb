{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "795c3527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import urllib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, fbeta_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa1b484",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94ce5027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted rows with conflicts: 40\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('PhiUSIIL_Phishing_URL_Dataset.csv')\n",
    "df2 = pd.read_csv('train.csv')\n",
    "\n",
    "df1 = df1[['URL', 'label']]\n",
    "df2 = df2.rename(columns={'url': 'URL', 'result': 'label'})\n",
    "\n",
    "df = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "duplicate_mask = df.duplicated(subset=['URL'], keep=False)\n",
    "conflicting_urls = df[duplicate_mask]['URL'].unique()\n",
    "df_clean = df[~df['URL'].isin(conflicting_urls)].reset_index(drop=True)\n",
    "print(f\"Deleted rows with conflicts: {len(df) - len(df_clean)}\")\n",
    "df = df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54396ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_urls = df.URL.values\n",
    "y = df.label.values\n",
    "\n",
    "X_temp, X_train_urls, y_temp, y_train = train_test_split(\n",
    "    X_urls, y, test_size=0.7, stratify=y, random_state=42\n",
    ")\n",
    "X_val_urls, X_test_urls, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cf8587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(s):\n",
    "    if not s:\n",
    "        return 0.0\n",
    "    prob = [s.count(c) / len(s) for c in set(s)]\n",
    "    return -sum(p * math.log2(p) for p in prob)\n",
    "\n",
    "def rule_based_phish(url):\n",
    "    return '@' in url\n",
    "\n",
    "def extract_features(url):\n",
    "    has_scheme_orig = url.startswith(('http://', 'https://'))\n",
    "    parse_url = url if has_scheme_orig else 'http://' + url\n",
    "    \n",
    "    try:\n",
    "        parsed = urllib.parse.urlparse(parse_url)\n",
    "        netloc = parsed.netloc\n",
    "        \n",
    "        if netloc.startswith('xn--'):\n",
    "            netloc = netloc.encode('ascii').decode('idna')\n",
    "        \n",
    "        normalized_url = parsed._replace(netloc=netloc).geturl()\n",
    "        url = normalized_url\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    has_scheme = url.startswith(('http://', 'https://'))\n",
    "    parse_url = url if has_scheme else 'http://' + url\n",
    "    \n",
    "    try:\n",
    "        parsed = urllib.parse.urlparse(parse_url)\n",
    "        netloc = parsed.netloc.lower()\n",
    "        path = parsed.path.lower()\n",
    "    except:\n",
    "        netloc = path = \"\"\n",
    "    \n",
    "    num_slashes = url.count('/')\n",
    "    num_exclam = url.count('!')\n",
    "    num_at = url.count('@')\n",
    "    num_dollar = url.count('$')\n",
    "    num_dots = netloc.count('.')\n",
    "    len_netloc = len(netloc)\n",
    "    dot_density = num_dots / (len_netloc + 1e-6)\n",
    "    len_url = len(url)\n",
    "    path_depth = max(0, num_slashes - 2)\n",
    "    \n",
    "    parts = netloc.split('.') if netloc else []\n",
    "    subdomain = '.'.join(parts[:-2]) if len(parts) > 2 else \"\"\n",
    "    is_random_subdomain = (\n",
    "        len(subdomain) > 10 and \n",
    "        subdomain.isalnum() and \n",
    "        not any(brand in subdomain for brand in ['www', 'mail', 'blog'])\n",
    "    )\n",
    "    \n",
    "    random_strings = re.findall(r'[a-zA-Z0-9]{10,}', path)\n",
    "    has_random_string_in_path = len(random_strings) > 0\n",
    "    \n",
    "    clean_netloc = netloc.replace('.', '').replace('-', '')\n",
    "    domain_entropy = entropy(clean_netloc) if clean_netloc else 0.0\n",
    "    \n",
    "    part_lengths = [len(part) for part in parts] if parts else [0]\n",
    "    domain_length_std = np.std(part_lengths) if len(part_lengths) > 1 else 0.0\n",
    "    \n",
    "    digit_count = sum(c.isdigit() for c in netloc)\n",
    "    digit_ratio = digit_count / (len(netloc) + 1e-6)\n",
    "    \n",
    "    return np.array([\n",
    "        num_slashes,\n",
    "        num_exclam,\n",
    "        num_at,\n",
    "        num_dollar,\n",
    "        dot_density,\n",
    "        len_netloc,\n",
    "        len_url,\n",
    "        path_depth,\n",
    "        int(is_random_subdomain),\n",
    "        int(has_random_string_in_path),\n",
    "        domain_entropy,\n",
    "        domain_length_std,\n",
    "        digit_ratio\n",
    "    ], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfa3e934",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256\n",
    "CHARS = \"\".join(sorted(set(\n",
    "    \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    \"0123456789\"\n",
    "    \"-._~\"\n",
    "    \":/?#[]@!$&'()*+,;=\"\n",
    "    \"`{}|\\\\^%\\\"<> \"\n",
    ")))\n",
    "char_to_id = {ch: i + 1 for i, ch in enumerate(CHARS)}\n",
    "\n",
    "def url_to_seq(url, max_len=MAX_LEN):\n",
    "    if not isinstance(url, str):\n",
    "        url = \"\"\n",
    "    url = url.lower()\n",
    "    seq = [char_to_id.get(c, 0) for c in url[:max_len]]\n",
    "    return seq + [0] * (max_len - len(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e075864b",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d524a11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridDataset(Dataset):\n",
    "    def __init__(self, urls, labels=None, scaler=None, fit_scaler=False):\n",
    "        self.urls = urls\n",
    "        self.labels = labels\n",
    "        \n",
    "        self.seqs = np.array([url_to_seq(url) for url in urls], dtype=np.int64)\n",
    "        \n",
    "        hc_features = np.array([extract_features(url) for url in urls], dtype=np.float32)\n",
    "        if fit_scaler:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.hc_features = self.scaler.fit_transform(hc_features)\n",
    "        elif scaler is not None:\n",
    "            self.hc_features = scaler.transform(hc_features)\n",
    "        else:\n",
    "            self.hc_features = hc_features\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.urls)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.seqs[idx]\n",
    "        hc = self.hc_features[idx]\n",
    "        if self.labels is not None:\n",
    "            y = float(self.labels[idx])\n",
    "            return (torch.tensor(seq), torch.tensor(hc)), torch.tensor(y, dtype=torch.float32)\n",
    "        return (torch.tensor(seq), torch.tensor(hc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ad7128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HybridDataset(X_train_urls, y_train, fit_scaler=True)\n",
    "scaler = train_dataset.scaler\n",
    "val_dataset = HybridDataset(X_val_urls, y_val, scaler=scaler)\n",
    "test_dataset = HybridDataset(X_test_urls, y_test, scaler=scaler)\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf23891a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(scaler, 'scaler.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a327abfe",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08ce8ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridPhishNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, kernel_sizes=[3, 4, 5, 6, 7, 8], num_filters=128, \n",
    "                 handcrafted_dim=13, hidden_cnn=128, hidden_hc=8, hidden_combined=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        # CNN\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(embed_dim, num_filters, k),\n",
    "                nn.ReLU(),\n",
    "                nn.AdaptiveMaxPool1d(1)\n",
    "            ) for k in kernel_sizes\n",
    "        ])\n",
    "        self.cnn_classifier = nn.Linear(num_filters * len(kernel_sizes), hidden_cnn)\n",
    "        \n",
    "        # Handcrafted\n",
    "        self.hc_net = nn.Sequential(\n",
    "            nn.Linear(handcrafted_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(16, hidden_hc)\n",
    "        )\n",
    "        \n",
    "        # Joining\n",
    "        self.combined_net = nn.Sequential(\n",
    "            nn.Linear(hidden_cnn + hidden_hc, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_combined, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_seq, x_hc):\n",
    "        # CNN\n",
    "        emb = self.embedding(x_seq).permute(0, 2, 1)\n",
    "        cnn_features = torch.cat([conv(emb).squeeze(-1) for conv in self.convs], dim=1)\n",
    "        cnn_out = self.cnn_classifier(cnn_features)\n",
    "        \n",
    "        # Handcrafted\n",
    "        hc_out = self.hc_net(x_hc)\n",
    "        \n",
    "        # Joining\n",
    "        combined = torch.cat([cnn_out, hc_out], dim=1)\n",
    "        return self.combined_net(combined).squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4706f7",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c352139a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for (x_seq, x_hc), y in dataloader:\n",
    "        x_seq, x_hc, y = x_seq.to(device), x_hc.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_seq, x_hc)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for (x_seq, x_hc), y in dataloader:\n",
    "            x_seq, x_hc, y = x_seq.to(device), x_hc.to(device), y.to(device)\n",
    "            outputs = model(x_seq, x_hc)\n",
    "            all_preds.append(outputs.cpu().numpy())\n",
    "            all_labels.append(y.cpu().numpy())\n",
    "    return np.concatenate(all_preds), np.concatenate(all_labels)\n",
    "\n",
    "def find_best_threshold_f2(y_true, y_proba):\n",
    "    best_thresh = 0.5\n",
    "    best_f2 = 0.0\n",
    "    for thresh in np.arange(0.01, 0.5, 0.01):\n",
    "        y_pred = (y_proba > thresh).astype(int)\n",
    "        f2 = fbeta_score(y_true, y_pred, beta=2.0, pos_label=1)\n",
    "        if f2 > best_f2:\n",
    "            best_f2 = f2\n",
    "            best_thresh = thresh\n",
    "    print(f\"Best F2 threshold: {best_thresh:.2f} - F2: {best_f2:.4f}\")\n",
    "    return best_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d004637",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "PATIENCE = 7\n",
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "731a46e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HybridPhishNet(vocab_size=len(CHARS) + 1).to(DEVICE)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=2, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba36980c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 661337\n"
     ]
    }
   ],
   "source": [
    "total_params = 0\n",
    "for param in model.parameters():\n",
    "    total_params += param.numel()\n",
    "\n",
    "print(f\"Total params: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9004e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | Loss: 0.1676 | Val F2: 0.9534\n",
      "Epoch  2 | Loss: 0.1072 | Val F2: 0.9561\n",
      "Epoch  3 | Loss: 0.0874 | Val F2: 0.9600\n",
      "Epoch  4 | Loss: 0.0713 | Val F2: 0.9696\n",
      "Epoch  5 | Loss: 0.0622 | Val F2: 0.9608\n",
      "Epoch  6 | Loss: 0.0538 | Val F2: 0.9619\n",
      "Epoch  7 | Loss: 0.0472 | Val F2: 0.9605\n",
      "Epoch  8 | Loss: 0.0257 | Val F2: 0.9654\n",
      "Epoch  9 | Loss: 0.0209 | Val F2: 0.9701\n",
      "Epoch 10 | Loss: 0.0163 | Val F2: 0.9716\n",
      "Epoch 11 | Loss: 0.0141 | Val F2: 0.9684\n",
      "Epoch 12 | Loss: 0.0115 | Val F2: 0.9686\n",
      "Epoch 13 | Loss: 0.0110 | Val F2: 0.9643\n",
      "Epoch 14 | Loss: 0.0052 | Val F2: 0.9718\n",
      "Epoch 15 | Loss: 0.0043 | Val F2: 0.9721\n",
      "Epoch 16 | Loss: 0.0036 | Val F2: 0.9683\n",
      "Epoch 17 | Loss: 0.0041 | Val F2: 0.9683\n",
      "Epoch 18 | Loss: 0.0029 | Val F2: 0.9656\n",
      "Epoch 19 | Loss: 0.0019 | Val F2: 0.9654\n",
      "Epoch 20 | Loss: 0.0018 | Val F2: 0.9675\n",
      "Epoch 21 | Loss: 0.0019 | Val F2: 0.9697\n",
      "Epoch 22 | Loss: 0.0015 | Val F2: 0.9662\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "best_f2 = 0\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "    val_preds, val_labels = evaluate(model, val_loader, DEVICE)\n",
    "    \n",
    "    current_f2 = fbeta_score(val_labels, (val_preds > 0.3).astype(int), beta=2.0, pos_label=1)\n",
    "    scheduler.step(current_f2)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d} | Loss: {train_loss:.4f} | Val F2: {current_f2:.4f}\")\n",
    "    \n",
    "    if current_f2 > best_f2:\n",
    "        best_f2 = current_f2\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_hybrid_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(\"Early stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa8faa4",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8472df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F2 threshold: 0.06 - F2: 0.9729\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "val_preds, val_labels = evaluate(model, val_loader, DEVICE)\n",
    "best_thresh = find_best_threshold_f2(val_labels, val_preds)\n",
    "test_preds, test_labels = evaluate(model, test_loader, DEVICE)\n",
    "\n",
    "test_pred_final = []\n",
    "for i, url in enumerate(X_test_urls):\n",
    "    if rule_based_phish(url):\n",
    "        test_pred_final.append(1) \n",
    "    else:\n",
    "        test_pred_final.append(int(test_preds[i] > best_thresh))\n",
    "\n",
    "test_pred_final = np.array(test_pred_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "884ae079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:        0.9812\n",
      "Precision:     0.9355\n",
      "F1-score:      0.9578\n",
      "F2-score:      0.9717\n"
     ]
    }
   ],
   "source": [
    "test_recall = recall_score(test_labels, test_pred_final, pos_label=1)\n",
    "test_precision = precision_score(test_labels, test_pred_final, pos_label=1)\n",
    "test_f1 = f1_score(test_labels, test_pred_final, pos_label=1)\n",
    "test_f2 = fbeta_score(test_labels, test_pred_final, beta=2.0, pos_label=1)\n",
    "\n",
    "print(f\"Recall:        {test_recall:.4f}\")\n",
    "print(f\"Precision:     {test_precision:.4f}\")\n",
    "print(f\"F1-score:      {test_f1:.4f}\")\n",
    "print(f\"F2-score:      {test_f2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
